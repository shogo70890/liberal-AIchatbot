import logging
"""
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ç”»é¢è¡¨ç¤ºä»¥å¤–ã®æ§˜ã€…ãªé–¢æ•°å®šç¾©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚
"""

############################################################
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿
############################################################
import os
import streamlit as st

from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage
from langchain_openai import ChatOpenAI
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
import constants as ct

############################################################
# è¨­å®šé–¢é€£
############################################################
# --- APIã‚­ãƒ¼ç¢ºèªã¨è¨­å®š ---
key = st.secrets.get("OPENAI_API_KEY", "")
if not key.startswith("sk-"):
    st.error("OPENAI_API_KEY ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Settings â†’ Secrets ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()
os.environ["OPENAI_API_KEY"] = key  # å¿µã®ãŸã‚ç’°å¢ƒå¤‰æ•°ã«ã‚‚åæ˜ 


############################################################
# ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆBM25 + Chromaï¼‰
############################################################
# ===== ã“ã“ã‹ã‚‰è¿½è¨˜ï¼šãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆBM25 + Chromaï¼‰ =====
from typing import List, Iterable
from langchain.schema import Document
from rank_bm25 import BM25Okapi

def _normalize(text: str) -> str:
    """å–ã‚Šè¾¼ã¿/å•ã„åˆã‚ã›ã§åŒä¸€ã®æ­£è¦åŒ–ï¼ˆæœ€ä½é™ç‰ˆï¼‰"""
    import re
    t = text.strip()
    t = re.sub(r"\s+", " ", t)
    t = t.replace("ã€€", " ")  # å…¨è§’ç©ºç™½
    return t

def _char_ngrams(s: str, n: int = 2) -> List[str]:
    s = _normalize(s)
    if len(s) <= n:
        return [s] if s else []
    return [s[i:i+n] for i in range(len(s) - n + 1)]

class SimpleBM25Retriever:
    """æ—¥æœ¬èªå‘ã‘ï¼šæ–‡å­—2gramã§BM25ã€‚ä¾å­˜ã¯ rank_bm25 ã®ã¿ã€‚"""
    def __init__(self, docs: List[Document], k: int = 8, ngram: int = 2):
        self.docs = docs
        self.k = k
        self.ngram = ngram
        corpus_tokens = [_char_ngrams(d.page_content, n=self.ngram) for d in docs]
        self.bm25 = BM25Okapi(corpus_tokens)

    def get_relevant_documents(self, query: str) -> List[Document]:
        tokens = _char_ngrams(query, n=self.ngram)
        scores = self.bm25.get_scores(tokens)
        idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[: self.k]
        out = []
        for i in idxs:
            d = self.docs[i].copy()
            md = d.metadata or {}
            md["_bm25_score"] = float(scores[i])
            d.metadata = md
            out.append(d)
        return out

def _doc_identity_key(d: Document):
    m = d.metadata or {}
    return (m.get("source"), m.get("page"), m.get("id"), hash(d.page_content))

def rrf_fuse(result_lists: List[List[Document]], k: int = 8, k_rrf: int = 60) -> List[Document]:
    """RRFèåˆã€‚list of ranked docs ã‚’åˆç®—ã—ã¦ä¸Šä½kã‚’è¿”ã™ã€‚"""
    from collections import defaultdict
    score = defaultdict(float)
    last = {}
    for results in result_lists:
        for rank, doc in enumerate(results):
            key = _doc_identity_key(doc)
            last[key] = doc
            score[key] += 1.0 / (k_rrf + rank + 1)
    fused = sorted(score.items(), key=lambda kv: kv[1], reverse=True)[:k]
    return [last[key] for key, _ in fused]

class HybridRetriever:
    """BM25 + ãƒ™ã‚¯ã‚¿ãƒ¼(MMR) ã‚’RRFã§èåˆã€‚LangChainäº’æ›ã® get_relevant_documents ã ã‘å®Ÿè£…ã€‚"""
    def __init__(self, bm25_ret: SimpleBM25Retriever, vec_ret, k: int = 8, k_rrf: int = 60):
        self.bm25_ret = bm25_ret
        self.vec_ret = vec_ret
        self.k = k
        self.k_rrf = k_rrf

    def _is_short(self, q: str) -> bool:
        # ã–ã£ãã‚Šï¼šçŸ­ã„ãƒ»åè©ã ã‘ã®ä¸€èªãªã©ã‚’ãƒ–ãƒ¼ã‚¹ãƒˆå¯¾è±¡ã«
        return len(q.strip()) <= 8

    def get_relevant_documents(self, query: str) -> List[Document]:
        # çŸ­å•ã®ã¨ãã¯BM25å´ã‚’ã‚„ã‚„å¼·ã‚ã«
        if self._is_short(query):
            orig_k = self.bm25_ret.k
            self.bm25_ret.k = max(orig_k, self.k + 2)
            bm25_docs = self.bm25_ret.get_relevant_documents(query)
            self.bm25_ret.k = orig_k  # æˆ»ã™
            vec_docs = self.vec_ret.get_relevant_documents(query)
        else:
            bm25_docs = self.bm25_ret.get_relevant_documents(query)
            vec_docs = self.vec_ret.get_relevant_documents(query)

        fused = rrf_fuse([bm25_docs, vec_docs], k=self.k, k_rrf=self.k_rrf)
        return fused if fused else (bm25_docs or vec_docs)

def build_hybrid_retriever(vectorstore, docs: List[Document], k: int = 8):
    """æ—¢å­˜Chromaã‹ã‚‰ retriever ã‚’ä½œã‚Šã€BM25 ã¨èåˆã—ã¦è¿”ã™ã€‚"""
    vec_ret = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": k, "fetch_k": 80, "lambda_mult": 0.7})
    bm25_ret = SimpleBM25Retriever(docs, k=k, ngram=3)
    return HybridRetriever(bm25_ret, vec_ret, k=k, k_rrf=60)

# ===== è¿½è¨˜ã“ã“ã¾ã§ =====


############################################################
# é–¢æ•°å®šç¾©
############################################################

def get_source_icon(source):
    """
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€ç·’ã«è¡¨ç¤ºã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡ã‚’å–å¾—

    Args:
        source: å‚ç…§å…ƒã®ã‚ã‚Šã‹

    Returns:
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€ç·’ã«è¡¨ç¤ºã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡
    """
    # å‚ç…§å…ƒãŒWebãƒšãƒ¼ã‚¸ã®å ´åˆã¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã§ã€å–å¾—ã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡ã‚’å¤‰ãˆã‚‹
    if source.startswith("http"):
        icon = ct.LINK_SOURCE_ICON
    else:
        icon = ct.DOC_SOURCE_ICON
    
    return icon


def build_error_message(message):
    """
    ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµ

    Args:
        message: ç”»é¢ä¸Šã«è¡¨ç¤ºã™ã‚‹ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµãƒ†ã‚­ã‚¹ãƒˆ
    """
    return "\n".join([message, ct.COMMON_ERROR_MESSAGE])


def get_llm_response(chat_message):
    """
    LLMã‹ã‚‰ã®å›ç­”å–å¾—

    Args:
        chat_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤

    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    # LLMã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”¨æ„
    llm = ChatOpenAI(model=ct.MODEL, temperature=ct.TEMPERATURE)

    # ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚LLMã«ç†è§£ã—ã¦ã‚‚ã‚‰ãˆã‚‹ã€ç‹¬ç«‹ã—ãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    question_generator_template = ct.SYSTEM_PROMPT_CREATE_INDEPENDENT_TEXT
    question_generator_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_generator_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ]
    )

    # å•ã„åˆã‚ã›ãƒ¢ãƒ¼ãƒ‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    question_answer_template = ct.SYSTEM_PROMPT_INQUIRY
    # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    question_answer_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_answer_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ]
    )

    # ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚LLMã«ç†è§£ã—ã¦ã‚‚ã‚‰ãˆã‚‹ã€ç‹¬ç«‹ã—ãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®Retrieverã‚’ä½œæˆ
    history_aware_retriever = create_history_aware_retriever(
        llm, st.session_state.retriever, question_generator_prompt
    )

    # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®Chainã‚’ä½œæˆ
    question_answer_chain = create_stuff_documents_chain(llm, question_answer_prompt)
    # ã€ŒRAG x ä¼šè©±å±¥æ­´ã®è¨˜æ†¶æ©Ÿèƒ½ã€ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®Chainã‚’ä½œæˆ
    chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)


    # ğŸ”½ ã“ã“ã‹ã‚‰ â€œinvoke â†’ streamâ€ ã¸æœ€å°æ”¹ä¿®
    inputs = {"input": chat_message, "chat_history": st.session_state.chat_history}

    # 1) å…ˆã«æ–‡è„ˆï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰ã ã‘å–å¾—
    context_docs = history_aware_retriever.invoke(inputs)

    # 2) ã‚¹ãƒˆãƒªãƒ¼ãƒ ã§å›ç­”ã‚’æ›¸ãå‡ºã—
    placeholder = st.empty()
    answer_buf = ""

    # question_answer_chain ã¯ Runnable ãªã®ã§ .stream ãŒä½¿ãˆã‚‹
    for chunk in question_answer_chain.stream({**inputs, "context": context_docs}):
        # LangChainã®streamã¯dictãƒãƒ£ãƒ³ã‚¯ã‚’è¿”ã™ã€‚answerã‚­ãƒ¼ã ã‘æ‹¾ã£ã¦æ›´æ–°
        if isinstance(chunk, dict) and "answer" in chunk:
            answer_buf += chunk["answer"]
            # å…¥åŠ›ä¸­ã‚«ãƒ¼ã‚½ãƒ«ã£ã½ã â–Œ ã‚’ä»˜ã‘ã‚‹ã¨é›°å›²æ°—ãŒå‡ºã‚‹
            placeholder.markdown(answer_buf + "â–Œ")

    # 3) æœ€çµ‚æç”»ï¼ˆã‚«ãƒ¼ã‚½ãƒ«ã‚’æ¶ˆã™ï¼‰
    placeholder.markdown(answer_buf)

    # 4) äº’æ›ã®æˆ»ã‚Šå€¤ã‚’çµ„ã¿ç«‹ã¦ï¼ˆå¾“æ¥ã® llm_response ã£ã½ãï¼‰
    llm_response = {"answer": answer_buf, "context": context_docs}


    # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
    st.session_state.chat_history.extend([HumanMessage(content=chat_message), llm_response["answer"]])

    return llm_response